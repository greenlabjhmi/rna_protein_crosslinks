{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Copyright (C) 2022 Allen Buskirk\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from subprocess import Popen\n",
    "from multiprocessing import Pool \n",
    "from time import sleep, time\n",
    "import signal\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pysam\n",
    "from Bio.Seq import Seq\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths/parameters\n",
    "fastq_dir = \"/home/allen/data/4suv/ribo_seq/\"\n",
    "output_dir = \"/home/allen/data/4suv/\"\n",
    "available_cores = 48 # must use fewer threads than samples\n",
    "star_ncRNA_dir = \"/home/allen/annotations/hg38_ensemble/ncRNA_STAR/\"\n",
    "star_genome_dir = \"/home/allen/annotations/hg38_ensemble/hg38_STAR/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exist\n",
      "Ready to process files:\n",
      "\t\tRIBO_UV_R1.fastq.gz\n",
      "\t\tRIBO_4SU_R3.fastq.gz\n",
      "\t\tRIBO_4SU_UV_R2.fastq.gz\n",
      "\t\tRIBO_UV_R3.fastq.gz\n",
      "\t\tRIBO_UV_R2.fastq.gz\n",
      "\t\tRIBO_4SU_R2.fastq.gz\n",
      "\t\tRIBO_UNT_R2.fastq.gz\n",
      "\t\tRIBO_4SU_R1.fastq.gz\n",
      "\t\tRIBO_UNT_R1.fastq.gz\n",
      "\t\tRIBO_4SU_UV_R3.fastq.gz\n",
      "\t\tRIBO_4SU_UV_R1.fastq.gz\n",
      "\t\tRIBO_UNT_R3.fastq.gz\n",
      "\n",
      "Using 4/48 cores for each sample.\n",
      "Data will be output to /home/allen/data/4suv/\n"
     ]
    }
   ],
   "source": [
    "#set up folder structure for output\n",
    "if os.access(output_dir, os.F_OK):\n",
    "    if os.access(output_dir+\"logs\", os.F_OK):\n",
    "        print(\"Directories already exist\")\n",
    "    else:\n",
    "        os.chdir(output_dir)\n",
    "        os.mkdir(output_dir+\"logs\")\n",
    "        #os.mkdir(output_dir+\"/output/deduplicated\")\n",
    "        os.mkdir(output_dir+\"trimmed\")\n",
    "        os.mkdir(output_dir+\"ncRNA_aligned\")\n",
    "        os.mkdir(output_dir+\"genome_aligned\")\n",
    "else:\n",
    "    print(\"WARNING:\", output_dir, \"does not exist.\")\n",
    "if not os.access(output_dir, os.F_OK):\n",
    "    print(\"WARNING:\", fastq_dir, \"does not exist.\")\n",
    "\n",
    "#check file endings for correct data processing.\n",
    "filenames = os.listdir(fastq_dir)\n",
    "fastq_files = []\n",
    "for filename in filenames:\n",
    "    if filename.endswith(\".fastq\") or filename.endswith(\".fastq.gz\"):\n",
    "        fastq_files.append(filename)\n",
    "\n",
    "#calculate the number of cores by dividing available cores by number of samples.\n",
    "numcores = max(available_cores//len(fastq_files), 1)\n",
    "\n",
    "print(\"Ready to process files:\")\n",
    "[print(\"\\t\\t\"+file) for file in fastq_files]\n",
    "print(\"\\nUsing\", str(numcores)+\"/\"+str(available_cores), \"cores for each sample.\")\n",
    "print(\"Data will be output to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines process handler for running one sample through the pipeline\n",
    "class run_one_sample():\n",
    "    '''handles the running of one sample through the entire pipeline. \n",
    "    This class should be called by an individual thread. It will output stdout/stderr \n",
    "    readouts from the individual steps in the .../output/logs/ folder.\n",
    "    \n",
    "    It also uses a simple text log to pick up where the sample left off if the pipeline \n",
    "    fails. This log is stored as completion_log.txt\n",
    "    Steps are defined in self.steps\n",
    "    \n",
    "    The self.check method will terminate the run and kill all running process groups if\n",
    "    one of the processes in this thread returns an exit code other than 0, or if any\n",
    "    other thread changes the global kill_pipeline variable to True. Default behavior is \n",
    "    to set kill_pipeline to True whenever any thread fails, thus halting processing of \n",
    "    all samples.\n",
    "    \n",
    "    On completion, each thread will add one to the global samples_done counter, to \n",
    "    let the main thread know when all are finished.  \n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        global kill_pipeline\n",
    "        global samples_done\n",
    "        global star_ncRNA_dir\n",
    "        global star_genome_dir\n",
    "        global fastq_dir\n",
    "        global output_dir\n",
    "        global numcores\n",
    "        global failed_sample\n",
    "        global thread_tracker\n",
    "        self.star_ncRNA_dir = star_ncRNA_dir\n",
    "        self.star_genome_dir = star_genome_dir\n",
    "        self.fastq_dir = fastq_dir\n",
    "        self.filename = filename\n",
    "        self.output_dir = output_dir\n",
    "        self.numcores = numcores\n",
    "        self.killflag = False\n",
    "        match = re.search(\"(.+)(\\.fastq|\\.fastq\\.gz)$\", self.filename)\n",
    "        self.prefix = match.group(1)\n",
    "        self.suffix = match.group(2)\n",
    "        if self.prefix not in thread_tracker:\n",
    "            thread_tracker[self.prefix] = \"initiate\"\n",
    "        self.steps = self.define_steps()\n",
    "        \n",
    "        with open(output_dir+\"logs/\"+self.prefix+\"_stdout.txt\", \"a\") as self.stdout, \\\n",
    "            open(output_dir+\"logs/\"+self.prefix+\"_completion_log.txt\", \"a\") as self.completion:\n",
    "            try:\n",
    "                for step_string in list(self.steps.keys()):\n",
    "                    if self.killflag == False and kill_pipeline == False:\n",
    "                        function_name, function_args = self.steps.pop(step_string)\n",
    "                        self.process = function_name(*function_args)\n",
    "                        self.check(self.process)\n",
    "                        self.completion.write(step_string+\"\\n\")\n",
    "                        self.completion.flush()\n",
    "                        thread_tracker[self.prefix] = step_string\n",
    "                if len(self.steps) == 0:\n",
    "                    samples_done += 1\n",
    "                        \n",
    "            except:\n",
    "                self.killflag = True\n",
    "                kill_pipeline = True\n",
    "                samples_done += 1 \n",
    "                raise\n",
    "                \n",
    "    \n",
    "    def define_steps(self):\n",
    "        steps = OrderedDict([\n",
    "            #(\"deduplicate\", (self.deduplicate, (fastq_dir, output_dir, self.filename, numcores))),\n",
    "            (\"trim\", (self.trim_reads, (fastq_dir, output_dir, self.filename, numcores))),\n",
    "            (\"align_ncRNA\", (self.align_ncRNA, (output_dir, self.filename, numcores))),\n",
    "            (\"align_genome\", (self.align_genome, (output_dir, self.filename, numcores))),\n",
    "            (\"sort_genome_aligned\", (self.samtools_sort, (\"genome_aligned\", numcores))),\n",
    "            (\"index_genome_aligned\", (self.samtools_index, (\"genome_aligned\", numcores))),\n",
    "        ])\n",
    "            \n",
    "        if os.access(output_dir+\"logs/\"+self.prefix+\"_completion_log.txt\", os.F_OK):\n",
    "            with open(output_dir+\"logs/\"+self.prefix+\"_completion_log.txt\", \"r\") as self.completion:\n",
    "                for line in self.completion.readlines():\n",
    "                    steps.pop(line.strip())\n",
    "        return steps\n",
    "    \n",
    "    \n",
    "    def deduplicate(self, fastq_dir, output_dir, filename, numcores):\n",
    "        '''deduplicate ribosome profiling reads using dedupe.sh from \n",
    "        the BBTools suite.\n",
    "        '''\n",
    "        self.stdout.write(\"Deduplicate\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        dedupe_dir = output_dir+\"deduplicated/\"+self.prefix+\"/\"\n",
    "        if not os.access(dedupe_dir, os.F_OK):\n",
    "            os.mkdir(dedupe_dir)\n",
    "        return Popen([\n",
    "            \"/home/allen/code/bbmap/dedupe.sh\",\n",
    "            \"in=\"+fastq_dir+filename,\n",
    "            \"out=\"+dedupe_dir+self.prefix+\".deduped\"+self.suffix,\n",
    "            \"absorbmatch=t\", #absorb exact matches of contigs\n",
    "            \"absorbcontainment=f\", #do not absorb full containments of contigs\n",
    "            \"absorbrc=f\", #do not absorb reverse-compliments\n",
    "            \"threads=\"+str(numcores),\n",
    "            \"overwrite=t\",\n",
    "        ], stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def trim_reads(self, fastq_dir, output_dir, filename, numcores):\n",
    "        '''Trim adapters and low quality regions from reads using bbduk.sh\n",
    "        from the BBTools suite.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nTrim Reads\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "        if not os.access(trimmed_dir, os.F_OK):\n",
    "            os.mkdir(trimmed_dir)\n",
    "            os.mkdir(trimmed_dir+\"failedQC\")\n",
    "        return Popen([\n",
    "            \"/home/allen/code/bbmap/bbduk.sh\",\n",
    "            \"in=\"+fastq_dir+filename,\n",
    "            \"out=\"+trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "            \"outm=\"+trimmed_dir+\"failedQC/\"+self.prefix+\".failedQC\"+self.suffix,\n",
    "            \"rpkm=\"+trimmed_dir+\"rpkm.txt\",\n",
    "            \"refstats=\"+trimmed_dir+\"trimming_stats.txt\",\n",
    "#             \"literal=CTGTAGGCACCATCAATATCTCGTATGCCGTCTTCTGCTTGAAAA\",   ### used for israeli FP data\n",
    "            \"literal=NNNNNNCACTCGGGCACCAAGGAC\", ### usual boris v 2 linker\n",
    "            \"k=24\", # this parameter sets the minimum kmer being trimmed. Longer = more specific, shorter = more sensitive\n",
    "            \"mink=8\", #includes truncations of the kmers down to 8\n",
    "            \"mm=f\", #do not ignore middle base mismatch of kmer\n",
    "            \"rcomp=f\", #do not allow reverse complement kmer matches\n",
    "            \"copyundefined=t\",\n",
    "            \"ktrim=r\",\n",
    "            \"forcetrimleft=4\", #removes random barcode on left of reads.\n",
    "            \"minavgquality=10\",\n",
    "            \"minlength=10\",\n",
    "            \"threads=\"+str(numcores),\n",
    "            \"overwrite=t\",\n",
    "        ],\n",
    "        stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "\n",
    "    def align_ncRNA(self, output_dir, filename, numcores):\n",
    "        '''Align reads to ncRNA using STAR. ncRNA fasta sequences from Boris.\n",
    "        Output unaligned reads.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nAlign to ncRNA\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "        ncRNA_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "        if not os.access(ncRNA_aligned_dir, os.F_OK):\n",
    "            os.mkdir(ncRNA_aligned_dir)\n",
    "        command = [\n",
    "            \"STAR\",\n",
    "            \"--runThreadN\", str(numcores),\n",
    "            \"--genomeDir\", self.star_ncRNA_dir,\n",
    "            \"--readFilesIn\", trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "            \"--outFileNamePrefix\", ncRNA_aligned_dir+self.prefix+\"_\",\n",
    "            \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "            \"--outReadsUnmapped\", \"Fastx\",\n",
    "            \"--alignSJDBoverhangMin\", \"1\",\n",
    "            \"--alignSJoverhangMin\", \"8\",\n",
    "            \"--outFilterMultimapNmax\", \"20\",\n",
    "            \"--outFilterType\", \"BySJout\",\n",
    "        ]\n",
    "        return Popen(command, stderr=self.stdout, stdout=self.stdout, \n",
    "                     preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def align_genome(self, output_dir, filename, numcores):\n",
    "        '''Align remaining reads to genome.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nAlign to genome\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        previous_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "        tx_aligned_dir = output_dir+\"genome_aligned/\"+self.prefix+\"/\"\n",
    "        if not os.access(tx_aligned_dir, os.F_OK):\n",
    "            os.mkdir(tx_aligned_dir)\n",
    "        command = [\n",
    "            \"STAR\",\n",
    "            \"--runThreadN\", str(self.numcores),\n",
    "            \"--genomeDir\", self.star_genome_dir,\n",
    "            \"--readFilesIn\", previous_aligned_dir+self.prefix+\"_Unmapped.out.mate1\",\n",
    "            \"--outFileNamePrefix\", tx_aligned_dir+self.prefix+\"_\",\n",
    "            \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "            \"--outSAMattributes\", \"All\",\n",
    "            \"--outReadsUnmapped\", \"Fastx\",\n",
    "            \"--alignSJDBoverhangMin\", \"1\",\n",
    "            \"--alignSJoverhangMin\", \"8\",\n",
    "            \"--outFilterMultimapNmax\", \"1\", #how many multimap sites allowed for read\n",
    "            \"--outSAMmultNmax\", \"1\", #how many map sites to write to output for each read\n",
    "            \"--outMultimapperOrder\", \"Random\", #assign read to random alignment if multimapper\n",
    "            \"--outFilterType\", \"BySJout\",\n",
    "        ]\n",
    "        return Popen(command, stderr=self.stdout, stdout=self.stdout,\n",
    "                     preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def samtools_sort(self, input_dir, numcores):\n",
    "        '''Sort BAM file from STAR ouput.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nSort \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        aligned_suffix = \"_Aligned.out\"\n",
    "        return Popen([\n",
    "            \"samtools\",\n",
    "            \"sort\",\n",
    "            \"-@\", str(numcores),\n",
    "            self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",\n",
    "            \"-o\", \n",
    "            self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "        ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def samtools_index(self, input_dir, numcores):\n",
    "        '''Index BAM file from STAR output\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nIndex \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        aligned_suffix = \"_Aligned.out\"\n",
    "        try:\n",
    "            os.remove(self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        return Popen([\n",
    "            \"samtools\",\n",
    "            \"index\",\n",
    "            \"-@\", str(numcores),\n",
    "            self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "        ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "               \n",
    "    def check(self, proc):\n",
    "        '''Poll Popen processes returned by each method to determine if a nonzero\n",
    "        error code was returned. If so, kill process group and set global kill_pipeline\n",
    "        to True, signalling to other processing threads to shut down as well.\n",
    "        Polling happens every 1 second.\n",
    "        '''\n",
    "        global kill_pipeline\n",
    "        global thread_tracker\n",
    "        exit_code = proc.poll()\n",
    "        while exit_code == None:\n",
    "            sleep(1)\n",
    "            if kill_pipeline == True:\n",
    "                self.killflag = True\n",
    "            if self.killflag == True:\n",
    "                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
    "            exit_code = proc.poll()\n",
    "        else:\n",
    "            if exit_code != 0:\n",
    "                self.killflag = True\n",
    "                kill_pipeline = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Running...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"37a31454-6f70-4598-b94a-dfec07de43d6\" data-root-id=\"25563\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  const docs_json = {\"28047225-93d2-4a61-83f1-eec77fdf8b0d\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"background_fill_color\":\"lightgray\",\"below\":[{\"id\":\"25574\"}],\"center\":[{\"id\":\"25576\"},{\"id\":\"25579\"}],\"height\":200,\"left\":[{\"id\":\"25577\"}],\"renderers\":[{\"id\":\"25586\"}],\"title\":{\"id\":\"25564\"},\"toolbar\":{\"id\":\"25580\"},\"width\":800,\"x_range\":{\"id\":\"25566\"},\"x_scale\":{\"id\":\"25570\"},\"y_range\":{\"id\":\"25568\"},\"y_scale\":{\"id\":\"25572\"}},\"id\":\"25563\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"25600\",\"type\":\"AllLabels\"},{\"attributes\":{\"factors\":[\"Start\",\"Trimming Adapter\",\"Aligning to ncRNA\",\"Aligning to Genome\",\"Sorting genome aligned reads\",\"Indexing Genome Aligned Reads\",\"Done\"]},\"id\":\"25566\",\"type\":\"FactorRange\"},{\"attributes\":{\"axis\":{\"id\":\"25577\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null,\"visible\":false},\"id\":\"25579\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#1f77b4\"},\"hatch_alpha\":{\"value\":0.2},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"25585\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"25603\",\"type\":\"AllLabels\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"25602\"},\"group\":null,\"major_label_orientation\":0.39269908169872414,\"major_label_policy\":{\"id\":\"25603\"},\"ticker\":{\"id\":\"25575\"}},\"id\":\"25574\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"25580\",\"type\":\"Toolbar\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"25599\"},\"group\":null,\"major_label_policy\":{\"id\":\"25600\"},\"ticker\":{\"id\":\"25578\"}},\"id\":\"25577\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"25578\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"25572\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"axis\":{\"id\":\"25574\"},\"coordinates\":null,\"grid_line_color\":\"gray\",\"group\":null,\"ticker\":null},\"id\":\"25576\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"25581\"}},\"id\":\"25587\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"25575\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"25570\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"data\":{\"height\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],\"left\":[0,0,0,0,0,0,0,0,0,0,0,0],\"right\":[\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\",\"Start\"],\"y\":[\"RIBO_UV_R1\",\"RIBO_4SU_R3\",\"RIBO_4SU_UV_R2\",\"RIBO_UV_R3\",\"RIBO_UV_R2\",\"RIBO_4SU_R2\",\"RIBO_UNT_R2\",\"RIBO_4SU_R1\",\"RIBO_UNT_R1\",\"RIBO_4SU_UV_R3\",\"RIBO_4SU_UV_R1\",\"RIBO_UNT_R3\"]},\"selected\":{\"id\":\"25605\"},\"selection_policy\":{\"id\":\"25604\"}},\"id\":\"25581\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"factors\":[\"RIBO_UV_R1\",\"RIBO_4SU_R3\",\"RIBO_4SU_UV_R2\",\"RIBO_UV_R3\",\"RIBO_UV_R2\",\"RIBO_4SU_R2\",\"RIBO_UNT_R2\",\"RIBO_4SU_R1\",\"RIBO_UNT_R1\",\"RIBO_4SU_UV_R3\",\"RIBO_4SU_UV_R1\",\"RIBO_UNT_R3\"]},\"id\":\"25568\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"25604\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"25599\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"25583\",\"type\":\"HBar\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"25581\"},\"glyph\":{\"id\":\"25583\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"25585\"},\"nonselection_glyph\":{\"id\":\"25584\"},\"view\":{\"id\":\"25587\"}},\"id\":\"25586\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"25602\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text\":\"Pipeline Progress\"},\"id\":\"25564\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"25605\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"hatch_alpha\":{\"value\":0.1},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"25584\",\"type\":\"HBar\"}],\"root_ids\":[\"25563\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n",
       "  const render_items = [{\"docid\":\"28047225-93d2-4a61-83f1-eec77fdf8b0d\",\"notebook_comms_target\":\"25606\",\"root_ids\":[\"25563\"],\"roots\":{\"25563\":\"37a31454-6f70-4598-b94a-dfec07de43d6\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "25563"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline finished successfully!\n",
      "Run time: 28.49 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "kill_pipeline = False\n",
    "samples_done = 0\n",
    "sample_runs = {}\n",
    "thread_tracker = OrderedDict()\n",
    "\n",
    "#start threads for running each sample through the pipeline\n",
    "for filename in fastq_files:\n",
    "    sample_runs[filename] = Thread(target=run_one_sample, args=(filename,))\n",
    "    sample_runs[filename].start()\n",
    "    \n",
    "print(\"Pipeline Running...\")\n",
    "\n",
    "steps_mapper = OrderedDict([ #offset by one because if it's recorded in the log, it's already done\n",
    "    (\"initiate\", \"Trimming Adapter\"),\n",
    "    (\"trim\", \"Aligning to ncRNA\"),\n",
    "    (\"align_ncRNA\", \"Aligning to Genome\"), \n",
    "    (\"align_genome\", \"Sorting genome aligned reads\"), \n",
    "    (\"sort_genome_aligned\", \"Indexing Genome Aligned Reads\"), \n",
    "    (\"index_genome_aligned\", \"Done\"),\n",
    "])\n",
    "\n",
    "steps = [\"Start\"]+[steps_mapper[step] for step in steps_mapper]\n",
    "prefixes = list(thread_tracker.keys())\n",
    "\n",
    "p = figure(height=200, width=800, y_range=prefixes, background_fill_color=\"lightgray\", x_range=steps, title=\"Pipeline Progress\", tools=[])\n",
    "p.xgrid.grid_line_color = \"gray\"\n",
    "p.xaxis.major_label_orientation = np.pi/8\n",
    "\n",
    "p.ygrid.visible = False\n",
    "source = ColumnDataSource(data={\"y\":prefixes, \"right\":[\"Start\"]*len(prefixes), \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)})\n",
    "x = p.hbar(y=\"y\", right=\"right\", height=\"height\", left=\"left\", source=source)\n",
    "\n",
    "show(p, notebook_handle=True)\n",
    "\n",
    "#Check every 1 sec whether the pipeline is finished and report whether it's been terminated.\n",
    "try:\n",
    "    while samples_done != len(fastq_files):\n",
    "        for sample in thread_tracker:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "        sleep(1)\n",
    "    else:\n",
    "        if kill_pipeline == False:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "            print(\"Pipeline finished successfully!\")\n",
    "        else:\n",
    "            print(\"Run terminated, check for errors\")\n",
    "except:\n",
    "    kill_pipeline = True  #allows KeyboardInterrupt to kill pipeline\n",
    "    print(\"Run terminated. Check for errors.\")\n",
    "    raise\n",
    "\n",
    "    \n",
    "\n",
    "runtime = time() - start_time\n",
    "if runtime > 60:\n",
    "    mins = round(runtime/60, 2)\n",
    "    print(\"Run time:\", mins, \"minutes\")\n",
    "else:\n",
    "    secs = round(runtime, 2)\n",
    "    print(\"Run time:\", secs, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIBO_UV_R1\n",
      "Total reads =  47214512\n",
      "47101731 or 99.76% survived trimming.\n",
      "36658111 or 77.83% mapped to ncRNA.\n",
      "4584715 or 9.73% mapped to the genome.\n",
      "5858905 or 12.44% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_R3\n",
      "Total reads =  52474177\n",
      "52359691 or 99.78% survived trimming.\n",
      "38880650 or 74.26% mapped to ncRNA.\n",
      "6244789 or 11.93% mapped to the genome.\n",
      "7234252 or 13.82% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_UV_R2\n",
      "Total reads =  30129632\n",
      "29691889 or 98.55% survived trimming.\n",
      "20758828 or 69.91% mapped to ncRNA.\n",
      "3663567 or 12.34% mapped to the genome.\n",
      "5269494 or 17.75% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_UV_R3\n",
      "Total reads =  53916147\n",
      "53816938 or 99.82% survived trimming.\n",
      "41202232 or 76.56% mapped to ncRNA.\n",
      "6164841 or 11.46% mapped to the genome.\n",
      "6449865 or 11.98% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_UV_R2\n",
      "Total reads =  53331014\n",
      "53031408 or 99.44% survived trimming.\n",
      "38651826 or 72.88% mapped to ncRNA.\n",
      "7269189 or 13.71% mapped to the genome.\n",
      "7110393 or 13.41% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_R2\n",
      "Total reads =  49419441\n",
      "49353643 or 99.87% survived trimming.\n",
      "34903359 or 70.72% mapped to ncRNA.\n",
      "6380633 or 12.93% mapped to the genome.\n",
      "8069651 or 16.35% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_UNT_R2\n",
      "Total reads =  55611894\n",
      "55536621 or 99.86% survived trimming.\n",
      "41495733 or 74.72% mapped to ncRNA.\n",
      "7099733 or 12.78% mapped to the genome.\n",
      "6941155 or 12.5% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_R1\n",
      "Total reads =  39910169\n",
      "39762049 or 99.63% survived trimming.\n",
      "28412379 or 71.46% mapped to ncRNA.\n",
      "5593754 or 14.07% mapped to the genome.\n",
      "5755916 or 14.48% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_UNT_R1\n",
      "Total reads =  45927877\n",
      "45849268 or 99.83% survived trimming.\n",
      "35250782 or 76.88% mapped to ncRNA.\n",
      "5084822 or 11.09% mapped to the genome.\n",
      "5513664 or 12.03% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_UV_R3\n",
      "Total reads =  53664784\n",
      "53508232 or 99.71% survived trimming.\n",
      "40643445 or 75.96% mapped to ncRNA.\n",
      "5343471 or 9.99% mapped to the genome.\n",
      "7521316 or 14.06% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_4SU_UV_R1\n",
      "Total reads =  64845110\n",
      "63944090 or 98.61% survived trimming.\n",
      "42148473 or 65.91% mapped to ncRNA.\n",
      "9127661 or 14.27% mapped to the genome.\n",
      "12667956 or 19.81% remained unmapped.\n",
      "\n",
      "\n",
      "\n",
      "RIBO_UNT_R3\n",
      "Total reads =  61032353\n",
      "60948087 or 99.86% survived trimming.\n",
      "45597272 or 74.81% mapped to ncRNA.\n",
      "7752590 or 12.72% mapped to the genome.\n",
      "7598225 or 12.47% remained unmapped.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_fate(sample_name):\n",
    "    global output_dir\n",
    "    with open(output_dir+\"logs/\"+sample_name+\"_stdout.txt\", \"r\", errors='ignore') as log, \\\n",
    "    open(output_dir+\"ncRNA_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as ncRNA_log, \\\n",
    "    open(output_dir+\"genome_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as genome_log:\n",
    "        file = log.readlines()[::-1]\n",
    "#         dedupe_end = file.index(\"Trim Reads\\n\")\n",
    "#         dedupe_start = file.index(\"--------------------\\n\", dedupe_end)\n",
    "#         for line in file[dedupe_end:dedupe_start]:\n",
    "#             if line.startswith(\"Input\"):\n",
    "#                 input_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "#             elif line.startswith(\"Result\"):\n",
    "#                 deduplicated_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "        trim_end = file.index(\"Align to ncRNA\\n\")\n",
    "        trim_start = file.index(\"--------------------\\n\", trim_end)\n",
    "        for line in file[trim_end:trim_start]:\n",
    "            if line.startswith(\"Input:\"):\n",
    "                input_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "            if line.startswith(\"Result:\"):\n",
    "                trimmed_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "        file = ncRNA_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                ncRNA_mapped_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                ncRNA_mapped_reads += int(line.split(\"|\")[1].strip())\n",
    "        file = genome_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Number of input reads\"):\n",
    "                genome_input_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                genome_mapped = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                genome_mapped += int(line.split(\"|\")[1].strip())\n",
    "        unmapped_reads = genome_input_reads - genome_mapped\n",
    "#         print(\"Library was\", str(round(deduplicated_reads/input_reads*100, 2))+\"%\", \"unique.\")\n",
    "#         print(\"Of those,\", str(round(trimmed_reads/deduplicated_reads*100, 2))+\"%\", \"survived trimming.\")\n",
    "        print(\"Total reads = \", str(input_reads))\n",
    "        print(str(trimmed_reads)+' or', str(round(trimmed_reads/input_reads*100, 2))+\"%\", \"survived trimming.\")\n",
    "        print(str(ncRNA_mapped_reads)+' or',str(round(ncRNA_mapped_reads/trimmed_reads*100, 2))+\"%\", \"mapped to ncRNA.\")\n",
    "        print(str(genome_mapped)+' or',str(round(genome_mapped/trimmed_reads*100, 2))+\"%\", \"mapped to the genome.\")\n",
    "        print(str(unmapped_reads)+' or',str(round(unmapped_reads/trimmed_reads*100, 2))+\"%\", \"remained unmapped.\")\n",
    "\n",
    "fn = []\n",
    "for i in filenames:\n",
    "    if \"RIBO\" in i:\n",
    "        fn.append(i)        \n",
    "        \n",
    "for fname in fn:\n",
    "    print(fname[:-9])\n",
    "    read_fate(fname[:-9])   # cuts off .fastq.gz\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making density files for each of these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse GTF\n",
    "\n",
    "with open(\"/home/allen/annotations/hg38_ensemble/files/Homo_sapiens.GRCh38.105.gtf\", \"r\") as inGTF:\n",
    "\n",
    "    gtf_dict = {\"chrom\":[], \"feature\":[], \"start\":[], \"end\":[], \n",
    "                \"gene_id\":[], \"transcript_id\":[], \"exon_number\":[], \n",
    "                \"strand\":[], \"annotation_type\":[], \"support_level\":[],\n",
    "                \"biotype\":[]\n",
    "               }\n",
    "    for line in inGTF:\n",
    "        try:\n",
    "            if not line.startswith(\"#\"):\n",
    "                splitline = line.split(\"\\t\")\n",
    "                chrom = splitline[0]\n",
    "                feature = splitline[2]\n",
    "                if feature == \"stop_codon\": \n",
    "                    feature = \"CDS\"\n",
    "                if feature not in [\"transcript\", \"CDS\", \"exon\"] or chrom not in [str(num) for num in range(1,24)]+[\"X\", \"Y\"]:\n",
    "                    continue\n",
    "\n",
    "                if not 'tag \"basic\"' in line:\n",
    "                    continue\n",
    "                start = int(splitline[3])\n",
    "                end = int(splitline[4])\n",
    "                gene_id = splitline[8].split(\"gene_id \")[1].split('\"')[1]\n",
    "                transcript_v = splitline[8].split(\"transcript_version \")[1].split('\"')[1]\n",
    "                transcript_id = splitline[8].split(\"transcript_id \")[1].split('\"')[1]+'.'+transcript_v                    \n",
    "                biotype = splitline[8].split(\"transcript_biotype \")[1].split('\"')[1]\n",
    "                try:\n",
    "                    support_level = splitline[8].split(\"transcript_support_level\")[1].split('\"')[1]\n",
    "                except IndexError:\n",
    "                    support_level = np.nan\n",
    "                annotation_type = splitline[1]\n",
    "                if feature == \"exon\":\n",
    "                    exon_number = int(splitline[8].split(\"exon_number \")[1].split('\"')[1])\n",
    "                else:\n",
    "                    exon_number = np.nan\n",
    "                strand = splitline[6]\n",
    "                gtf_dict[\"chrom\"].append(chrom)\n",
    "                gtf_dict[\"feature\"].append(feature)\n",
    "                gtf_dict[\"start\"].append(start)\n",
    "                gtf_dict[\"end\"].append(end)\n",
    "                gtf_dict[\"gene_id\"].append(gene_id)\n",
    "                gtf_dict[\"transcript_id\"].append(transcript_id)\n",
    "                gtf_dict[\"exon_number\"].append(exon_number)\n",
    "                gtf_dict[\"strand\"].append(strand)\n",
    "                gtf_dict[\"annotation_type\"].append(annotation_type)\n",
    "                gtf_dict[\"support_level\"].append(support_level)\n",
    "                gtf_dict[\"biotype\"].append(biotype)\n",
    "        except:\n",
    "            print(line)\n",
    "            raise\n",
    "gtf_df = pd.DataFrame(gtf_dict)\n",
    "\n",
    "transcripts_deduped = gtf_df[(gtf_df.feature == \"transcript\") & (gtf_df.biotype == \"protein_coding\")\n",
    "                            ].sort_values(by=[\"chrom\", \"gene_id\", \"support_level\", \"transcript_id\", \"start\"]\n",
    "                                         ).drop_duplicates(subset=[\"gene_id\"], keep=\"first\"\n",
    "                                                          ).transcript_id\n",
    "gtf_df = gtf_df[gtf_df.transcript_id.isin(transcripts_deduped)\n",
    "               ].sort_values(by=[\"chrom\", \"gene_id\", \"support_level\", \"transcript_id\", \"start\"])\n",
    "\n",
    "gtf_df[\"length\"] = gtf_df.end.apply(int) - gtf_df.start.apply(int) + 1\n",
    "max_gene = max(gtf_df.length)\n",
    "\n",
    "print(\"GTF loaded:\", len(transcripts_deduped), \"transcripts\")\n",
    "print(\"Filtered gtf_df down to\", len(gtf_df[gtf_df.feature == \"transcript\"]), \"matching transcripts.\")\n",
    "\n",
    "filename = \"./parsed_gtf.pkl\"\n",
    "pickle.dump(gtf_df, open(filename, \"wb\"))\n",
    "print(\"Pickling gtf_df to\",filename)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled gtf_df loaded: 19654 transcripts\n"
     ]
    }
   ],
   "source": [
    "### loading the parsed GTF pickle just created \n",
    "\n",
    "pickled_gtf_df = '/home/allen/code/4suv/github_4suv/parsed_gtf.pkl' \n",
    "gtf_df = pickle.load(open(pickled_gtf_df, \"rb\"))\n",
    "print(\"Pickled gtf_df loaded:\", len(gtf_df[gtf_df.feature == \"transcript\"]), \"transcripts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled tx_df loaded: 19580 transcripts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>chrom</th>\n",
       "      <th>strand</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>spliced_len</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000367770.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>48</td>\n",
       "      <td>2276</td>\n",
       "      <td>2916</td>\n",
       "      <td>CTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000286031.10</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>700</td>\n",
       "      <td>3261</td>\n",
       "      <td>4355</td>\n",
       "      <td>GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000374004.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>146</td>\n",
       "      <td>1735</td>\n",
       "      <td>2350</td>\n",
       "      <td>GGGAGGACCCCAATCTAGGCCCAAGAGGGAAAGCCACGTGCCTGTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000359637.2</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>62</td>\n",
       "      <td>1219</td>\n",
       "      <td>1454</td>\n",
       "      <td>GAGGAGAACTGGACGTTGTGAACAGAGTTAGCTGGTAAATGTCCTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000374409.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>255</td>\n",
       "      <td>1259</td>\n",
       "      <td>2804</td>\n",
       "      <td>TCGTCACAGCCATGAGTGAGACTTGAAGCCCGTTTTACGTATGAAG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        transcript_id chrom strand  start  stop  spliced_len  \\\n",
       "0   ENST00000367770.5     1      -     48  2276         2916   \n",
       "1  ENST00000286031.10     1      +    700  3261         4355   \n",
       "2   ENST00000374004.5     1      -    146  1735         2350   \n",
       "3   ENST00000359637.2     1      +     62  1219         1454   \n",
       "4   ENST00000374409.5     1      -    255  1259         2804   \n",
       "\n",
       "                                                 seq  \n",
       "0  CTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAG...  \n",
       "1  GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...  \n",
       "2  GGGAGGACCCCAATCTAGGCCCAAGAGGGAAAGCCACGTGCCTGTA...  \n",
       "3  GAGGAGAACTGGACGTTGTGAACAGAGTTAGCTGGTAAATGTCCTC...  \n",
       "4  TCGTCACAGCCATGAGTGAGACTTGAAGCCCGTTTTACGTATGAAG...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load pickled transcript database with information for translation \n",
    "\n",
    "tx_df = pickle.load(open(\"/home/allen/code/4suv/github_4suv/tx_df_pkl\", \"rb\"))\n",
    "print(\"Pickled tx_df loaded:\", len(tx_df), \"transcripts\")\n",
    "tx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## final function to assign density, called by wrapping function elsewhere (see below)\n",
    "\n",
    "pd.options.mode.chained_assignment = None #default = 'warn'\n",
    "\n",
    "'''\n",
    "Takes in a transcript line of gtf_df and creates a tx_density object.\n",
    "These will then be saved to a pkl file for fast parsing of downstream calculations.\n",
    "'''\n",
    "\n",
    "def get_density(transcript):\n",
    "\n",
    "    global gtf_df\n",
    "    global bamfile\n",
    "     \n",
    "    chrom = transcript[1].chrom ## comes as a tuple -- take [1]\n",
    "    strand = transcript[1].strand\n",
    "    transcript_id = transcript[1].transcript_id\n",
    "    tx_cds_exons_df = gtf_df[gtf_df.transcript_id == transcript_id] #forms a dataframe transcript, CDS, exon from gtf_df\n",
    "\n",
    "    tx = tx_cds_exons_df[tx_cds_exons_df.feature == \"transcript\"] #transcript line of dataframe\n",
    "    tx_left = tx.start.iloc[0] #Leftmost genome coordinate of transcript (could be 3' or 5' end depending on strand)\n",
    "    tx_right = tx.end.iloc[0] #Rightmost genome coordinate of transcript\n",
    "\n",
    "    exons = tx_cds_exons_df[tx_cds_exons_df.feature == \"exon\"] #Exon lines of dataframe\n",
    "    tx_len = sum(exons.length) #length of spliced transcript feature\n",
    "\n",
    "    #creates a dataframe of all information on reads mapping anywhere in the unspliced transcript.\n",
    "    read_iter = bamfile.fetch(chrom, int(tx_left), int(tx_right), multiple_iterators=True)\n",
    "    read_dict = {\"strand\":[], \"length\":[], \"left_pos\":[], \"right_pos\":[], \"transcript_id\":[], \"seq\":[], \"read_id\":[]}\n",
    "    for read in read_iter:\n",
    "        if read.is_reverse:\n",
    "            s = '-'\n",
    "        else:\n",
    "            s = '+'\n",
    "        if s == strand:\n",
    "            read_dict[\"strand\"].append(s)\n",
    "            read_dict[\"length\"].append(read.query_length)\n",
    "            read_dict[\"left_pos\"].append(read.reference_start+1) #pysam fetches coordinates 0-based, +1 for 1-based list\n",
    "            read_dict[\"right_pos\"].append(read.reference_end) #this gives nt 1 past end, so -1 to get end and +1 for 1 based list\n",
    "            read_dict[\"transcript_id\"].append(transcript_id)\n",
    "            inseq = read.query_sequence\n",
    "            if s == '-':\n",
    "                inseq = str(Seq(inseq).reverse_complement())\n",
    "            read_dict['seq'].append(inseq)\n",
    "            read_dict[\"read_id\"].append(read.query_name)\n",
    "\n",
    "    read_df = pd.DataFrame.from_dict(read_dict) \n",
    "\n",
    "    exonic_positions = [] #pd.Series to translate genome coords to spliced transcript coords\n",
    "    for exon in exons.iterrows():\n",
    "        exonic_positions.extend(np.arange(exon[1].start, exon[1].end+1))\n",
    "    exonic_positions = pd.Series(np.arange(0,len(exonic_positions)), index=exonic_positions) # 0 based list\n",
    "\n",
    "    #only select reads that are contained entirely within exons\n",
    "    exonic_reads = read_df.loc[\n",
    "        (read_df.left_pos.isin(exonic_positions.index)) & \n",
    "        (read_df.right_pos.isin(exonic_positions.index))]\n",
    "\n",
    "    # change from genome coords to exon transcript coords\n",
    "    # for - strand transcripts, the numbers are inverted so that everything is 5 to 3\n",
    "    if strand == \"+\":\n",
    "        exonic_reads.loc[:,(\"5p\")] = exonic_positions[exonic_reads.left_pos].values\n",
    "        exonic_reads.loc[:,(\"3p\")] = exonic_positions[exonic_reads.right_pos].values\n",
    "    elif strand == \"-\":\n",
    "        exonic_reads.loc[:,(\"5p\")] = tx_len-1-exonic_positions[exonic_reads.right_pos].values\n",
    "        exonic_reads.loc[:,(\"3p\")] = tx_len-1-exonic_positions[exonic_reads.left_pos].values\n",
    "\n",
    "    histo = np.histogram(exonic_reads['5p'], np.arange(0, tx_len+1, 1))[0]\n",
    "\n",
    "\n",
    "    return(histo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\n",
    " 'RIBO_4SU_R1',\n",
    " 'RIBO_4SU_R2',\n",
    " 'RIBO_4SU_R3',\n",
    " 'RIBO_4SU_UV_R1',\n",
    " 'RIBO_4SU_UV_R2',\n",
    " 'RIBO_4SU_UV_R3',\n",
    " 'RIBO_UNT_R1',\n",
    " 'RIBO_UNT_R2',\n",
    " 'RIBO_UNT_R3',\n",
    " 'RIBO_UV_R1',\n",
    " 'RIBO_UV_R2',\n",
    " 'RIBO_UV_R3'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled density file saved for: RIBO_4SU_UV_R1\n",
      "Pickled density file saved for: RIBO_4SU_UV_R3\n",
      "Pickled density file saved for: RIBO_UNT_R1\n",
      "Pickled density file saved for: RIBO_UV_R3\n",
      "Pickled density file saved for: RIBO_UNT_R2\n",
      "Pickled density file saved for: RIBO_UV_R2\n",
      "Pickled density file saved for: RIBO_4SU_R3\n",
      "Pickled density file saved for: RIBO_4SU_UV_R2\n",
      "Pickled density file saved for: RIBO_4SU_R2\n",
      "Pickled density file saved for: RIBO_UV_R1\n",
      "Pickled density file saved for: RIBO_UNT_R3\n",
      "Pickled density file saved for: RIBO_4SU_R1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fn in file_list:\n",
    "\n",
    "    bamfile = pysam.AlignmentFile(\"/home/allen/data/4suv/genome_aligned/\"\n",
    "        +fn+'/'+fn+\"_Aligned.out.sorted.bam\",\"rb\")\n",
    "\n",
    "    p = Pool(48)\n",
    "    map_out = p.map(get_density, tx_df.iterrows())\n",
    "    p.close()\n",
    "\n",
    "    density_dict = {}\n",
    "    for i in range(len(tx_df)):\n",
    "        density_dict[tx_df.iloc[i].transcript_id] = map_out[i]\n",
    "\n",
    "    pickle.dump(density_dict, open(\"/home/allen/data/4suv/density/\"+fn+\"_density_pkl\", \"wb\"))\n",
    "    print(\"Pickled density file saved for:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
